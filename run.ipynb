{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://zenn.dev/ryo_tan/articles/5d03c0157501aa\n",
    "\n",
    "import os\n",
    "import sys\n",
    "if not 'Informer' in sys.path:\n",
    "    sys.path += ['Informer']\n",
    "\n",
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from data.data_loader import Dataset_Pred\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def remove_directory(dir_path):\n",
    "    if os.path.exists(dir_path):\n",
    "        print('removing directory ', dir_path)\n",
    "        shutil.rmtree(dir_path)\n",
    "\n",
    "def make_directory(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        print('creating dir ', dir_path)\n",
    "        os.mkdir(dir_path)\n",
    "\n",
    "def plot_predictions(trues, preds, start_index, step, num_plots, setting, root_path):\n",
    "        num_rows = num_plots // 3\n",
    "        num_cols = 3\n",
    "\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(14, 3 * num_rows), sharex=True, sharey=True)\n",
    "\n",
    "        for i, ax in enumerate(axes.flatten()):\n",
    "            if i < num_plots:\n",
    "                index = start_index + i * step\n",
    "\n",
    "                ax.plot(trues[index, :, -1], label='GroundTruth')\n",
    "                ax.plot(preds[index, :, -1], label='Prediction')\n",
    "\n",
    "                ax.set_title(f'Index {index}')\n",
    "                ax.legend()\n",
    "                ax.set_xlabel('time')\n",
    "                ax.set_ylabel('Realized volatility')\n",
    "                ax.tick_params(axis='both', which='both', labelsize=8, direction='in')\n",
    "\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig(os.path.join(root_path, '/results_informer/'+setting+'/prediction_plot.png'))\n",
    "        \n",
    "\n",
    "def run_volatility(args):    \n",
    "    args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "    args.data = 'custom' # data\n",
    "    # args.root_path =  ROOT_DIR#'/content/' # root path of data file\n",
    "    args.freq = '1m' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "    args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
    "    args.seq_len = 96 # input sequence length of Informer encoder\n",
    "    args.label_len = 48 # start token length of Informer decoder\n",
    "    args.pred_len = 24 # prediction sequence length\n",
    "    # Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "    args.enc_in = 8 # encoder input size\n",
    "    args.dec_in = 8 # decoder input size\n",
    "    args.c_out = 8 # output size\n",
    "    args.factor = 1 # probsparse attn factor\n",
    "    args.d_model = 512 # dimension of model\n",
    "    args.n_heads = 8 # num of heads\n",
    "    args.d_ff = 2048 # dimension of fcn in model\n",
    "    args.dropout = 0.05 # dropout\n",
    "    args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "    args.embed = 'fixed' # time features encoding, options:[timeF, fixed, learned]\n",
    "    args.activation = 'gelu' # activation\n",
    "    args.distil = True # whether to use distilling in encoder\n",
    "    args.output_attention = False # whether to output attention in ecoder\n",
    "    args.mix = True\n",
    "    args.padding = 0\n",
    "    args.freq = 'h'\n",
    "    args.batch_size = 32\n",
    "    args.loss = 'mse'\n",
    "    args.lradj = 'type1'\n",
    "    args.use_amp = False # whether to use automatic mixed precision training\n",
    "    args.num_workers = 0\n",
    "    args.itr = 1\n",
    "    args.patience = 100\n",
    "    args.des = 'exp'\n",
    "    args.use_gpu = True if torch.cuda.is_available() else False\n",
    "    args.gpu = 0\n",
    "    args.use_multi_gpu = False\n",
    "    args.devices = '0,1,2,3'\n",
    "    args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "    args.inverse = True\n",
    "\n",
    "    if args.use_gpu and args.use_multi_gpu:\n",
    "        args.devices = args.devices.replace(' ','')\n",
    "        device_ids = args.devices.split(',')\n",
    "        args.device_ids = [int(id_) for id_ in device_ids]\n",
    "        args.gpu = args.device_ids[0]\n",
    "\n",
    "    # Set augments by using data name\n",
    "    data_parser = {\n",
    "        'custom':{'data':args.data_path,\n",
    "                'T':args.target,\n",
    "                'M': args.target_config_list_m,\n",
    "                'S':[1,1,1],\n",
    "                'MS': args.target_config_list_ms}, #Change the array here based on the number of features\n",
    "    }\n",
    "    if args.data in data_parser.keys():\n",
    "        data_info = data_parser[args.data]\n",
    "        args.data_path = data_info['data']\n",
    "        args.target = data_info['T']\n",
    "        args.enc_in, args.dec_in, args.c_out = data_info[args.features]\n",
    "\n",
    "    args.detail_freq = args.freq\n",
    "    args.freq = args.freq[-1:]\n",
    "\n",
    "    Exp = Exp_Informer\n",
    "    print('Args in experiment:')\n",
    "    print(args)\n",
    "\n",
    "    error_mertics = {}\n",
    "    losses = None\n",
    "    setting = ''\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(\n",
    "                    args.model_id,\n",
    "                    args.model, args.data, args.features,\n",
    "                    args.seq_len, args.label_len, args.pred_len,\n",
    "                    args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, \n",
    "                    args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "        # set experiments\n",
    "        exp = Exp(args)\n",
    "\n",
    "        # train\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        model, losses = exp.train(setting)\n",
    "\n",
    "        # test\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        error_mertics = exp.test(setting)\n",
    "\n",
    "        print('Finished training')\n",
    "\n",
    "    print('setting folder ', setting)\n",
    "    # If you already have a trained model, you can set the arguments and model path, then initialize a Experiment and use it to predict\n",
    "    # Prediction is a sequence which is adjacent to the last date of the data, and does not exist in the data\n",
    "    # If you want to get more information about prediction, you can refer to code `exp/exp_informer.py function predict()` and `data/data_loader.py class Dataset_Pred`\n",
    "\n",
    "    exp = Exp(args)\n",
    "\n",
    "    exp.predict(setting, True)\n",
    "\n",
    "    prediction = np.load('./results/'+setting+'/real_prediction.npy')\n",
    "\n",
    "    prediction.shape\n",
    "\n",
    "    Data = Dataset_Pred\n",
    "    timeenc = 0 if args.embed!='timeF' else 1\n",
    "    flag = 'pred'; shuffle_flag = False; drop_last = False; batch_size = 1\n",
    "\n",
    "    freq = args.detail_freq\n",
    "\n",
    "    data_set = Data(\n",
    "        root_path=args.root_path,\n",
    "        is_time_id=args.is_time_id,\n",
    "        data_path=args.data_path,\n",
    "        flag=flag,\n",
    "        size=[args.seq_len, args.label_len, args.pred_len],\n",
    "        features=args.features,\n",
    "        target=args.target,\n",
    "        timeenc=timeenc,\n",
    "        freq=freq\n",
    "    )\n",
    "    data_loader = DataLoader(\n",
    "        data_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_flag,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=drop_last)\n",
    "\n",
    "    len(data_set), len(data_loader)\n",
    "\n",
    "    # When we finished exp.train(setting) and exp.test(setting), we will get a trained model and the results of test experiment\n",
    "    # The results of test experiment will be saved in ./results/{setting}/pred.npy (prediction of test dataset) and ./results/{setting}/true.npy (groundtruth of test dataset)\n",
    "\n",
    "    preds = np.load('./results/'+setting+'/pred.npy')\n",
    "    trues = np.load('./results/'+setting+'/true.npy')\n",
    "\n",
    "    # [samples, pred_len, dimensions]\n",
    "    preds.shape, trues.shape\n",
    "\n",
    "    plot_predictions(trues, preds, start_index=0, step=50, num_plots=6, setting = setting, root_path=args.root_path)\n",
    "\n",
    "    return error_mertics, losses, setting\n",
    "\n",
    "def drawplots(epochs, train_loss, validation_loss, test_loss,title, setting, root_path):\n",
    "    plt.plot(epochs, train_loss, label=\"train\")\n",
    "    plt.plot(epochs, validation_loss, label=\"validation\")\n",
    "    plt.plot(epochs, test_loss, label=\"test\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(root_path, '/results_informer/' + setting + \"/\" + title + '.png'))\n",
    "    \n",
    "def drawplot(epochs, losses, title, setting, root_path):\n",
    "    plt.plot(epochs, losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(root_path, '/results_informer/' + setting + \"/\" + title + '.png'))\n",
    "    \n",
    "\n",
    "def run_experiments(run, run_type):\n",
    "    # run 1\n",
    "    args = dotdict()\n",
    "    args.target_config_list_ms = []\n",
    "    args.e_layers = 2 # num of encoder layers\n",
    "    args.d_layers = 1 # num of decoder layers\n",
    "    args.learning_rate = 0.00001 # 0.0001\n",
    "    args.train_epochs = 20\n",
    "    args.model_id = run_type + \"_\" + run\n",
    "    args.root_path = \"/Users/pujanmaharjan/uni adelaide/research project/realized-volatility/data\"\n",
    "\n",
    "    error_metrics_all = []\n",
    "    losses_all = []\n",
    "    setting = \"\"\n",
    "\n",
    "    # Run 1\n",
    "    if run == \"targets\":\n",
    "        args.data_path = 'stock_data_targets.csv' #'output.csv' # data file\n",
    "        args.target = 'stock_0' # target feature in S or MS task\n",
    "        args.features = 'M' # forecasting task, options:[M, S, MS];\n",
    "                                #M:multivariate predict multivariate, S:univariate predict univariate,\n",
    "                                #MS:multivariate predict univariate\n",
    "        m_feature_count = len(pd.read_csv(\"./dataset/stock_data_targets.csv\").columns) - 1\n",
    "        args.target_config_list_m = [m_feature_count, m_feature_count, m_feature_count]\n",
    "        args.is_time_id = True\n",
    "        \n",
    "        error_metrics_targets, losses_run_1, setting = run_volatility(args)\n",
    "        error_metrics_all.append(error_metrics_targets)\n",
    "        losses_all.append(losses_run_1)\n",
    "\n",
    "    # run 2\n",
    "    if run == \"tcn_targets\":\n",
    "        if run_type == \"similar\":\n",
    "            args.data_path = 'similar_stock_data_tcn_targets.csv'\n",
    "        elif run_type == \"dissimilar\":\n",
    "            args.data_path = 'dissimilar_stock_data_tcn_targets.csv'\n",
    "        else:\n",
    "            args.data_path ='stock_data_tcn_targets.csv'\n",
    "        args.target = 'stock_0_y' # target feature in S or MS task\n",
    "        args.features = 'M'\n",
    "        feature_count = len(pd.read_csv(os.path.join(args.root_path, args.data_path)).columns) - 1\n",
    "        args.target_config_list_m = [feature_count, feature_count, feature_count]\n",
    "        args.is_time_id = True\n",
    "        error_metrics_tcn, losses_run_2, setting = run_volatility(args)\n",
    "        error_metrics_all.append(error_metrics_tcn)\n",
    "        losses_all.append(losses_run_2)\n",
    "\n",
    "    # run 3\n",
    "    if run == \"features\":\n",
    "        args.data_path = 'stock_0_features.csv' #'output.csv' # data file\n",
    "        args.target = 'target' # target feature in S or MS task\n",
    "        args.features = 'MS'\n",
    "        args.target_config_list_ms = [9,9,1]\n",
    "        args.is_time_id = True\n",
    "        error_metrics_features, losses_run_3, setting = run_volatility(args)\n",
    "        error_metrics_all.append(error_metrics_features)\n",
    "        losses_all.append(losses_run_3)\n",
    "\n",
    "    print('error metrics all ', error_metrics_all)\n",
    "    error_metrics_df = pd.DataFrame(error_metrics_all)\n",
    "    print(error_metrics_df)\n",
    "    error_metrics_df.to_csv(os.path.join(args.root_path, '/results_informer/'+setting+'/error_metrics.csv', index=False))\n",
    "\n",
    "    losses_df = pd.DataFrame(losses_all[0])\n",
    "    losses_df.to_csv(os.path.join(args.root_path, '/results_informer/'+setting+'/losses.csv', index=False))\n",
    "\n",
    "    # print(losses)\n",
    "    first_loss = losses_all[0]\n",
    "    print(first_loss)\n",
    "    epochs = [f['epoch'] for f in first_loss]\n",
    "    print(epochs)\n",
    "    train_losses = [f['train_loss'] for f in first_loss]\n",
    "    validation_losses = [f['validation_loss'] for f in first_loss]\n",
    "    test_losses = [f['test_loss'] for f in first_loss]\n",
    "\n",
    "\n",
    "    drawplots(epochs, train_losses, validation_losses, test_losses, 'Loss curves', setting, args.root_path)\n",
    "    drawplot(epochs, train_losses, 'Train loss', setting, args.root_path)\n",
    "    drawplot(epochs, validation_losses, 'Validation loss', setting, args.root_path)\n",
    "    drawplot(epochs, test_losses, 'Test loss', setting, args.root_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'target_config_list_ms': [], 'e_layers': 2, 'd_layers': 1, 'learning_rate': 1e-05, 'train_epochs': 20, 'model_id': 'similar_tcn_targets', 'root_path': '/Users/pujanmaharjan/uni adelaide/research project/realized-volatility/data', 'data_path': 'similar_stock_data_tcn_targets.csv', 'target': 'stock_0_y', 'features': 'M', 'target_config_list_m': [12, 12, 12], 'is_time_id': True, 'model': 'informer', 'data': 'custom', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 24, 'enc_in': 12, 'dec_in': 12, 'c_out': 12, 'factor': 1, 'd_model': 512, 'n_heads': 8, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'fixed', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'patience': 100, 'des': 'exp', 'use_gpu': False, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'inverse': True, 'detail_freq': 'h'}\n",
      "using mps device\n",
      "Number of encoder layers  2\n",
      "Number of decoder layers,  1\n",
      ">>>>>>>start training : similar_tcn_targets_informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc1_ebfixed_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "custom data set\n",
      "data stamp shape  (2679, 1)\n",
      "data stamp first data  [1]\n",
      "train 2560\n",
      "custom data set\n",
      "data stamp shape  (480, 1)\n",
      "data stamp first data  [2584]\n",
      "val 361\n",
      "custom data set\n",
      "data stamp shape  (861, 1)\n",
      "data stamp first data  [2968]\n",
      "test 742\n",
      "Epoch: 1 cost time: 26.353116035461426\n",
      "Epoch: 1, Steps: 80 | Train Loss: 0.0089999 Vali Loss: 0.0092283 Test Loss: 0.0084568\n",
      "Validation loss decreased (inf --> 0.009228).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 25.28909993171692\n",
      "Epoch: 2, Steps: 80 | Train Loss: 0.0086914 Vali Loss: 0.0091757 Test Loss: 0.0083254\n",
      "Validation loss decreased (0.009228 --> 0.009176).  Saving model ...\n",
      "Updating learning rate to 5e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/pujanmaharjan/uni adelaide/research project/Informer/run.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pujanmaharjan/uni%20adelaide/research%20project/Informer/run.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# make_directory(results_path)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pujanmaharjan/uni%20adelaide/research%20project/Informer/run.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# tcn targets with similar nature of stocks\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pujanmaharjan/uni%20adelaide/research%20project/Informer/run.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m run_experiments(\u001b[39m\"\u001b[39;49m\u001b[39mtcn_targets\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msimilar\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/pujanmaharjan/uni adelaide/research project/Informer/run.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/pujanmaharjan/uni%20adelaide/research%20project/Informer/run.ipynb#W1sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m args\u001b[39m.\u001b[39mtarget_config_list_m \u001b[39m=\u001b[39m [feature_count, feature_count, feature_count]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/pujanmaharjan/uni%20adelaide/research%20project/Informer/run.ipynb#W1sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m args\u001b[39m.\u001b[39mis_time_id \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/pujanmaharjan/uni%20adelaide/research%20project/Informer/run.ipynb#W1sZmlsZQ%3D%3D?line=269'>270</a>\u001b[0m error_metrics_tcn, losses_run_2, setting \u001b[39m=\u001b[39m run_volatility(args)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/pujanmaharjan/uni%20adelaide/research%20project/Informer/run.ipynb#W1sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m error_metrics_all\u001b[39m.\u001b[39mappend(error_metrics_tcn)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/pujanmaharjan/uni%20adelaide/research%20project/Informer/run.ipynb#W1sZmlsZQ%3D%3D?line=271'>272</a>\u001b[0m losses_all\u001b[39m.\u001b[39mappend(losses_run_2)\n",
      "\u001b[1;32m/Users/pujanmaharjan/uni adelaide/research project/Informer/run.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/pujanmaharjan/uni%20adelaide/research%20project/Informer/run.ipynb#W1sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/pujanmaharjan/uni%20adelaide/research%20project/Informer/run.ipynb#W1sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>>>>>>start training : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(setting))\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/pujanmaharjan/uni%20adelaide/research%20project/Informer/run.ipynb#W1sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m model, losses \u001b[39m=\u001b[39m exp\u001b[39m.\u001b[39;49mtrain(setting)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/pujanmaharjan/uni%20adelaide/research%20project/Informer/run.ipynb#W1sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/pujanmaharjan/uni%20adelaide/research%20project/Informer/run.ipynb#W1sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>>>>>>>testing : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(setting))\n",
      "File \u001b[0;32m~/uni adelaide/research project/Informer/exp/exp_informer.py:158\u001b[0m, in \u001b[0;36mExp_Informer.train\u001b[0;34m(self, setting)\u001b[0m\n\u001b[1;32m    155\u001b[0m iter_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    157\u001b[0m model_optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 158\u001b[0m pred, true \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_one_batch(\n\u001b[1;32m    159\u001b[0m     train_data, batch_x, batch_y, batch_x_mark, batch_y_mark)\n\u001b[1;32m    160\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred, true)\n\u001b[1;32m    161\u001b[0m train_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/uni adelaide/research project/Informer/exp/exp_informer.py:293\u001b[0m, in \u001b[0;36mExp_Informer._process_one_batch\u001b[0;34m(self, dataset_object, batch_x, batch_y, batch_x_mark, batch_y_mark)\u001b[0m\n\u001b[1;32m    291\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(batch_x, batch_x_mark, dec_inp, batch_y_mark)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    292\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 293\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39minverse:\n\u001b[1;32m    295\u001b[0m     \u001b[39m# print('inverse transform output')\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     outputs \u001b[39m=\u001b[39m dataset_object\u001b[39m.\u001b[39minverse_transform(outputs)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/uni adelaide/research project/Informer/models/model.py:72\u001b[0m, in \u001b[0;36mInformer.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x_enc, x_mark_enc, x_dec, x_mark_dec, \n\u001b[1;32m     70\u001b[0m             enc_self_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dec_self_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dec_enc_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     71\u001b[0m     enc_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_embedding(x_enc, x_mark_enc)\n\u001b[0;32m---> 72\u001b[0m     enc_out, attns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(enc_out, attn_mask\u001b[39m=\u001b[39;49menc_self_mask)\n\u001b[1;32m     74\u001b[0m     dec_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec_embedding(x_dec, x_mark_dec)\n\u001b[1;32m     75\u001b[0m     dec_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(dec_out, enc_out, x_mask\u001b[39m=\u001b[39mdec_self_mask, cross_mask\u001b[39m=\u001b[39mdec_enc_mask)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/uni adelaide/research project/Informer/models/encoder.py:71\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     69\u001b[0m         x \u001b[39m=\u001b[39m conv_layer(x)\n\u001b[1;32m     70\u001b[0m         attns\u001b[39m.\u001b[39mappend(attn)\n\u001b[0;32m---> 71\u001b[0m     x, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_layers[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](x, attn_mask\u001b[39m=\u001b[39;49mattn_mask)\n\u001b[1;32m     72\u001b[0m     attns\u001b[39m.\u001b[39mappend(attn)\n\u001b[1;32m     73\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/uni adelaide/research project/Informer/models/encoder.py:44\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, attn_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     39\u001b[0m     \u001b[39m# x [B, L, D]\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[39m# x = x + self.dropout(self.attention(\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[39m#     x, x, x,\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[39m#     attn_mask = attn_mask\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[39m# ))\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     new_x, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m     45\u001b[0m         x, x, x,\n\u001b[1;32m     46\u001b[0m         attn_mask \u001b[39m=\u001b[39;49m attn_mask\n\u001b[1;32m     47\u001b[0m     )\n\u001b[1;32m     48\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(new_x)\n\u001b[1;32m     50\u001b[0m     y \u001b[39m=\u001b[39m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/uni adelaide/research project/Informer/models/attn.py:153\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[0;34m(self, queries, keys, values, attn_mask)\u001b[0m\n\u001b[1;32m    150\u001b[0m keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_projection(keys)\u001b[39m.\u001b[39mview(B, S, H, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    151\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_projection(values)\u001b[39m.\u001b[39mview(B, S, H, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m out, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_attention(\n\u001b[1;32m    154\u001b[0m     queries,\n\u001b[1;32m    155\u001b[0m     keys,\n\u001b[1;32m    156\u001b[0m     values,\n\u001b[1;32m    157\u001b[0m     attn_mask\n\u001b[1;32m    158\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmix:\n\u001b[1;32m    160\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/uni adelaide/research project/Informer/models/attn.py:114\u001b[0m, in \u001b[0;36mProbAttention.forward\u001b[0;34m(self, queries, keys, values, attn_mask)\u001b[0m\n\u001b[1;32m    111\u001b[0m U_part \u001b[39m=\u001b[39m U_part \u001b[39mif\u001b[39;00m U_part\u001b[39m<\u001b[39mL_K \u001b[39melse\u001b[39;00m L_K\n\u001b[1;32m    112\u001b[0m u \u001b[39m=\u001b[39m u \u001b[39mif\u001b[39;00m u\u001b[39m<\u001b[39mL_Q \u001b[39melse\u001b[39;00m L_Q\n\u001b[0;32m--> 114\u001b[0m scores_top, index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prob_QK(queries, keys, sample_k\u001b[39m=\u001b[39;49mU_part, n_top\u001b[39m=\u001b[39;49mu) \n\u001b[1;32m    116\u001b[0m \u001b[39m# add scale factor\u001b[39;00m\n\u001b[1;32m    117\u001b[0m scale \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale \u001b[39mor\u001b[39;00m \u001b[39m1.\u001b[39m\u001b[39m/\u001b[39msqrt(D)\n",
      "File \u001b[0;32m~/uni adelaide/research project/Informer/models/attn.py:56\u001b[0m, in \u001b[0;36mProbAttention._prob_QK\u001b[0;34m(self, Q, K, sample_k, n_top)\u001b[0m\n\u001b[1;32m     54\u001b[0m index_sample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(L_K, (L_Q, sample_k)) \u001b[39m# real U = U_part(factor*ln(L_k))*L_q\u001b[39;00m\n\u001b[1;32m     55\u001b[0m K_sample \u001b[39m=\u001b[39m K_expand[:, :, torch\u001b[39m.\u001b[39marange(L_Q)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), index_sample, :]\n\u001b[0;32m---> 56\u001b[0m Q_K_sample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(Q\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m), K_sample\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[39m# find the Top_k query with sparisty measurement\u001b[39;00m\n\u001b[1;32m     59\u001b[0m M \u001b[39m=\u001b[39m Q_K_sample\u001b[39m.\u001b[39mmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mdiv(Q_K_sample\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), L_K)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# make_directory(results_path)\n",
    "# tcn targets with similar nature of stocks\n",
    "run_experiments(\"tcn_targets\", \"similar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
